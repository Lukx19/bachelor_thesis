\chapter{Used algorithms and key concepts}
This section offers basic introduction to multiple state-of-the-art algorithms used in this work. This chapter starts with short explanation about what it is \gls{SLAM}. Then, it dives more deeply into Graph-based SLAM variant. Equally, important section \ref{MAP_REPRE} describes possible map representations. In next section \ref{Scan_reg} we introduce a selection of registration algorithms. At the end of this chapter \ref{subsec:analyses}, we will go over our reasons why there is need for this work and how to possibly improve current state-of-the-art in NDT based graph SLAM. 

\section{SLAM problem definition}
\label{sec:SLAM_def}
Successfully solving \gls{SLAM} problem means to find location of robot in every time step and be able to create map at that time-stamp. In real world we deal with robot's sensors which have always some inherited noise. This means we are not able to fully say exact position of robot. This is main reason why to use probabilistic definition of problem. Robot moves through unknown space along trajectory expressed as variables $ \textbf{x}_{1:T} = \{\textbf{x}_{1},...,\textbf{x}_{T}\} $. While moving robot is taking odometry measurements $ \textbf{u}_{1:T} = \{\textbf{u}_{1},...,\textbf{u}_{T}\}$ and perception of environment $ \textbf{z}_{1:T} = \{\textbf{z}_{1},...,\textbf{z}_{T}\}$ Solving SLAM than means finding out probability of the robot's trajectory $ \textbf{x}_{1:T}$ and a map \textbf{m} of local environment given all the measurements and initial pose $ \textbf{x}_{0}$:
\begin{equation}
p(\textbf{x}_{1:T}, \textbf{m}\: |\:  \textbf{z}_{1:T}, \textbf{u}_{1:T}, \textbf{x}_{0})
\end{equation}
Odometry is usually acquired by robots wheel encoders or by incremental scan-matching. Odometry is represented in 2D by triple $(x,y,\theta)$ or by three dimensional transformation matrix. Perceptions of environment might come from different sources. In this work we expect distance measurements from laser scanner or kinect sensor. Initial pose can be interpreted as origin of coordinate system for global map.The map can be represented as a set of unique landmarks or as a set of measurements integrated together. Some of these integration methods are presented in section \ref{MAP_REPRE}.

\subsection{SLAM categories}
Over the past decade reaserch has developed three distinctive categories of \gls{SLAM}. 

First category is \gls{EKF} variant. It is based on \gls{KF}. The \gls{KF}  assume that probability density functions are Gaussian distributions and system is linear. The \gls{EKF} solves problems with non-linearity of robot pose model. Performance of \gls{EKF} strongly depends on quality of statistical model for noise in sensors and odometry. Unfortunately, these models are usually not available. A set of comparative tests for convergence and inconsistencies of \gls{EKF} is in work of \cite{EKF}.

Another category is based on \gls{PF}. Current state of the robot is represented by set of weighted particles. This brings advantage of representing uncertainty through multi-modal distributions and dealing with non-Gaussian noise. \cite{FastSlam} proposed computationally efficient method based on \gls{PF} called FastSLAM. It uses particles to represent posterior probability of motion. In addition, each particle also holds K Kallman filters representing landmark positions. It was demonstrated that it is possible to calculate high precision maps utilizing FastSLAM. Inspired by FastSlam, a method based on Rao-Blackwellized Particle Filter is proposed in \cite{Rao-PF}. Derivations of this approach are still actively used in robotics today. 

Last category is based on modeling state of the system by constructing robot's state graph and optimizing to find final robot position. This representation was first time used in work of \cite{LuMilios}. This technique was later improved by \cite{OlsonGS}. They have presented efficient optimization approach based on the scholastic gradient descent. It was able to correct even large graphs. Later multiple authors improved \gls{SLAM} optimization by adding hierarchies to large graphs or adding robustness to optimization process. Graph based model of SLAM is still in active research.       

\subsection{Graph-based SLAM}
A graph-based SLAM solves SLAM problem by constructing graph representation of the problem. This graph is commonly called a pose graph. Nodes of the graph represent potential poses of robot at certain time stamp $ T $. Therefore, nodes are representing our trajectory $ \{\textbf{x}_{1},...,\textbf{x}_{T}\} $. Nodes also hold state of the current map. Edges in the graph represent possible spatial transformation between nodes. Edge is generated out of noisy sensor data. Therefore, we need to model uncertainty of this movement. It is represented by probability distribution and included in the edge. Generation of constrains is done by algorithm's front-end. It creates them either from odometry $  \textbf{u}_{T} $ or by measurement data  $ \textbf{z}_{T} $ registration. Once the graph is completed ,it is optimized by algorithms back-end. Result of this process is the most likely position of all nodes in the graph.

\subsection {Pose graph creation}
\label{Pose_graph_creation}
Process of graph creation operates in \gls{SLAM}'s front-end. First step is to receive robot's movement. This transformation may come from wheels' encoders, visual odometry from camera or \gls{IMU}. Front-end also receives a covariance of the transformation based on noise model of source sensor. From transformation and covariance we can create an edge for the graph. This edge type is usually called odometry edge. Consecutive odometry measurements creates long chain of edges in graph.

Nodes represent current robot position. Therefore, they should have some initial estimate. This initial guess may come from concatenation of transformations in odometry edges. Another method is to use propagation of transformation through minimum spanning tree constructed out of full graph. 

Second type, represents edges from nodes to landmarks. A landmark is and unique descriptors of the place. When landmark is detected, front-end creates node representing this place and landmark edge connecting it with graph. Edge carries transformation between current node and landmark. If landmark already exists than created edge might help to optimize correct pose estimate of other nodes.

   
Third common type of edges are loop closure edges. These edges usually connect two nodes, which share same perception of the world. Aligning these perceptions yields virtual transformation between these nodes. A covariance needs to be provided from alignment process and depends on used technique. Loop closure edge usually exists if we have revisited same place again. This is crucial information for \gls{SLAM}'s back-end. Based on it optimalization finds out if odometry edges reliably represent reality and adjust pose estimates.

\subsection {Loop closure creation}
First step of correct loop closure creation is to identify all nodes ,which might have overlapping measurements. Given pose $a$ we find all nodes $b_{1} ... b_{n}$ from graph whose sensor measurements overlap pose $a$. This could be determined by finding relative position of nodes $a$ and $b_{i}$. One possible method how to determine is to use Dijkstra projection mentioned in \cite{Olson2009Loop}. Dijkstra projection starts at node $a$ and concatenate covariances and transformation along the minimum uncertainty path. This path is selected based on determinant of covariance matrix. Small covariance matrix has lower determinant than covariance matrix with large numbers. Minimum uncertainty selection guaranties that algorithm will get to the target $b_{i}$ with maximum precision. Concatenation of covariances is done based on equation:
\begin{equation}
P_{a+b} = J_{a} P_{a} J_{a}^{T} + J_{b} P_{b} J_{b}^{T}
\end{equation}   
\begin{equation}
J_{a} = 
\begin{pmatrix}
1 & 0 & -x\sin\theta -y\cos\theta  \\
0 & 1 & x\cos \theta  - y\sin\theta  \\
0 & 0 & 1
\end{pmatrix} 
J_{b} = 
\begin{pmatrix}
\cos\theta & -\sin\theta & 0  \\
\sin\theta & \cos\theta & 0\\
0 & 0 & 1
\end{pmatrix} 
\end{equation} 
where $P_{a}$ is acumulated covariance, $P_{b}$ is aditional covariance, Jaccobian $J_{a}$ use parameters from transformation $(x,y,\theta)_{a}$ and $J_{b}$ from  $(x,y,\theta)_{b}$ Concatenation of transformations is defined as:
\begin{equation}
\begin{pmatrix}
x \\ y
\end{pmatrix}_{a+b}
=
\begin{pmatrix}
x \\ y
\end{pmatrix}_{a}
+ R(\theta_{a}) 
\begin{pmatrix}
x \\ y
\end{pmatrix}_{b}
\end{equation}
\begin{equation}
\theta_{a+b} = \theta_{a} + \theta_{b}
\end{equation}
where $R(\theta_{a})$ is rotation matrix created from angle $\theta_{a}$.

After successful generation of overlapping nodes, every potential pair needs to be tested by registration algorithm. This algorithm needs to be robust enough to reject as many incorrect pairs as possible. If matching is possible it should align measurements and return best transformation. More about this type of algorithms can be found in section \ref{Scan_reg}.

Even the best registration algorithm may fail and return erroneous measurement. Loop closure process needs to reject these errors. One solution is to use method proposed by \cite{Olson2009Loop}. In this approach we first group loop closure edges into groups based on their topological distance from each other. Later we validate every cluster against internal inconsistencies. Edges marked as inconsistent are deleted from system. 

Other option is to use robust optimization engines, witch can identify outliers in the form of error edges. Comparison of known outliers rejection methods was done by \cite{RobustOpt}. 



\subsection{Optimization}
Back-end receives graph with odometry edges and loop closure edges. The main task of back-end is to optimize this graph and return the most likely position of nodes. Popular method of optimization is to use the Gauss-Newton or the Levenberg-Marquardt algorithms. 

To utilize these methods we first need to define our error function. We will use notation similar to one presented in section \ref{sec:SLAM_def}. Let $\textbf{x} = (\textbf{x}_{1},...,\textbf{x}_{T})^{T} $ be a vector of graph's nodes positions. Let $z_{i,j}$ to be a registration algorithm transformation between nodes $x_{i}$ and $x_{j}$. Let $\Omega_{i,j}$ be a information matrix of this transformation (information matrix is an inverse of covariance). Lastly let $\hat{z}_{i,j}$ be a estimate of registration transform received from initial configurations of nodes i and j.

The log-likelihood of measurement $z_{i,j}$ is than defined as:
\begin{equation}
l_{i,j} = (\hat{z}_{i,j} - z_{i,j})^{T} \Omega_{i,j} (\hat{z}_{i,j} - z_{i,j}) 
\end{equation}  
where $(\hat{z}_{i,j} - z_{i,j})$ is a difference between expected measurement and real measurement. Now we can define out error function as

\begin{equation}
F(\textbf{x}_{1,T}) = \sum_{<i,j>\in G}^{} (\hat{z}_{i,j} - z_{i,j})^{T} \Omega_{i,j} (\hat{z}_{i,j} - z_{i,j}) 
\end{equation}

Our goal is to calculate such a $\textbf{x}$ that this function is minimal. More formaly we wan to find solution to
\begin{equation}
\bar{x}_{1,T} = argmin_{\textbf{x}} \ F(\textbf{x})
\end{equation}

 Information on how to minimize this function, calculate derivatives and how to exploit structure of the problem to get significant speed gains continue in reading in this tutorial \cite{GraphTutorial}.

\newpage


\section{Map representation}
\label{MAP_REPRE}
Successful solving SLAM problem should output map of the unknown environment. This map needs to be stored for local path planning and obstacle avoidance. It is also needed for scan registration. Algorithms for avoiding obstacles very often need precise map. Map should keep low memory consumption, because robots very often have limited access to memory. Scan registration algorithms usually might benefit from maps with high precision.

Point-cloud is map representation which stores all measured points. This is very precise representation. All input data is still in its raw form. We are not loosing any information. Scan-matching algorithms e.g. \gls{ICP} is working on top of this datastructure. It is very easy to convert from this model to different type of map if needed. Problematic is memory consumption. If robot runs for long period with higher frequency of data production, it is likely that robot will run out of memory.

Occupancy map is grid based type. It consists of grid with cells. In every cell we have just one value describing probability that this cell is occupied. Value becomes higher with more incoming data measurements. It has constant memory consumption with respect to time of robot's run-time. It is possible to use this representation for map to map registration process. This model is also possible to represent empty spaces (low probability). This feature is used by many path planning and obstacle avoidance algorithms. That is why, occupancy maps are main output format for SLAM algorithms in ROS. It is important to select good resolution of grid. Finer grid offers better detail but higher memory consumption.

Quad-tree is a tree data structure. Each node of the tree has exactly four children. Nodes are decomposing space into sub-areas. Every node has its threshold. When it is reached, cell subdivides into four smaller cells. This process dynamically change resolution of the grid. This way we get higher precision in places where it matters more. Maximal precision is usually bounded by minimal size of leaf nodes.

\gls{NDT} representation uses grid based datastructure. Each cell has normal distribution parameters calculated based on inserted points. This model offers constant memory consumption over time. In comparison, \gls{NDT} has better representation of inner points than octree (3D case of quad-tree). This was proven as convenient by \cite{Saarinen13}. They have shown that coarser \gls{NDT} grid can have similar results in precision of space representation than finer octree map. \gls{NDT} grids have their own class of registration whoch will be explained in next sections.     
\todo{picture of occupancy grid, pointcloud ndt grid}
\newpage


\subsection{NDT grid}
\label{subsec:NDT_grid}
The normal distributions transform(NDT) grid representation was first time used by \cite{Biber03} in their scan registration process. Central idea was to convert laser scan into grid with cells containing normal distributions. Points in space from laser scanner are first separated into corresponding cells. From points in sigle cell we approximate normal distribution $(\mu_{i},P_{i})$ by calculating mean and covariance:
\begin{equation}
\mu_{i} = \dfrac{1}{n}\sum_{k=1}^{n}x_{k}
\end{equation}  
\begin{equation}
P_{i} = \dfrac{1}{n-1}\sum_{k=1}^{n}(x_{k}-\mu_{i})(x_{k}-\mu_{i})^{t}
\end{equation} 
NDT grid was than used for registration.Originally proposed grid could be updated with new laser scans only by keeping used points and recalculating all cells again. This has changed with proposed recursive covariance update step by \cite{Saarinen13}. Their update step offers way how to fuse in new measurements. First it calculate normal distributions for added points. In second step, it merges old covariances with new one.

Consider two sets of measurement $\{x_{i}\}^{m}_{i=1}$ and $\{y_{i}\}^{n}_{i=1}$ than formula for mean calculation is in equation \eqref{NDT_mean_RCU}. \gls{RCU} is in equation \eqref{NDT_covar_RCU}
\begin{equation}
T_{x} = \sum_{i =1}^{m}x_{i} \quad
T_{y} = \sum_{i =1}^{n}y_{i} \quad
T_{x\oplus y} = T_{x} + T_{y} 
\end{equation}

\begin{equation}
\label{NDT_mean_RCU}
\mu_{x\oplus y} =\dfrac{1}{m + n}T_{x\oplus y}
\end{equation} 

\begin{equation}
S_{x} = \sum_{i=1}^{m}(x_{i} - \frac{1}{m}T_{x})(x_{i} - \frac{1}{m}T_{x})^{T} \quad 
S_{y} = \sum_{i=1}^{n}(y_{i} - \frac{1}{n}T_{y})(y_{i} - \frac{1}{n}T_{y})^{T}
\end{equation}
\begin{equation}
S_{x\oplus y} = S_{x} + S_{y} + \dfrac{m}{n(m+n)}(\frac{n}{m}T_{x} - T_{x\oplus y})(\frac{n}{m}T_{x} - T_{x\oplus y})^{T}
\end{equation}
\begin{equation}
\label{NDT_covar_RCU}
P_{x\oplus y} = \dfrac{1}{m+n -1}S_{x\oplus y}
\end{equation}

Proof and further explanation for these equations can be found in work of \cite{Saarinen13} and later improved in \cite{Saarinen213}.

In addition to fusing in new laser measurements we can also easily generated coarser grid by merging cells from higher resolution grid to grid with lower resolution. This mechanism is useful in path planning where we can plan on coarser grid which could be faster. Also, we can use multi-level scan matching approaches, which will be discussed in next section \ref{Scan_reg}. Small disadvantage of this method is that we need to keep number of points used in every cell.

It is worth noting that in continual integration of scans calculated mean and covariance grow unboundedwith increasing number of points added. This could lead to numerical instabilities. Second problem is that cell's distribution contains measurements from all scans. This is problem in dynamic environment where some objects might disappear. These problems are solved by restricting maximal number of points in cell with parameter M
\begin{equation}
N_{x \oplus y} = 
\begin{cases}
n+m, & n+m < M \\
M, & n+m \geq M
\end{cases} 
\end{equation}
Parameter M modifies how fast we let RCU replace old measurements by new one. Small value of M makes adaptation faster and big M keeps weight of older data higher. This cause to have new data making smaller impact on result of process. 

\subsection{NDT-OM extension}
NDT grids offers good compromise between space and precision, but it lacks information about occupied space and unoccupied space. This is crucial for planning algorithms. This functionality was added to NDT by \cite{Saarinen13} and later improved by same authors in later work \cite{Saarinen213}. Every cell in \gls{NDT-OM} is represented with parameters $c_{i}=\{\mu_{i}, p_{i}, N_{i},p_{i}\}$, where $\mu_{i}$ and $P_{i}$ are parameters of estimated normal distribution, $N_{i}$ is number of points in cell and $p_{i}$ is probability of the cell being occupied. 

Calculation of occupancy parameter is done by ray-tracing. Consider that we have current map $m_{x}$. We have calculated new NDT map $m_{y}$ from incoming distance measurements. Both maps needs to be in the same coordinate system. Ray-tracing starts at current robot position in map $m_{x}$. End point of ray-tracing is value of mean from one of the cells in new map $m_{y}$. Program visits every cell along the line and updates covariance. It is important to visit every cell just once. When is ray-tracing over we merge in all cells from $m_{y}$ into $m_{x}$ with RCU update rule.

The main idea in occupancy update calculation is that not all cells are occupied fully. Normal distribution usually occupies only part of the cell. A ray tracing through this cell might not intersect bounds of normal distribution at all. In order to consistently update occupancy the update value should not be a constant. Better option is to choose a function describing difference between map $m_{y}$ and $m_{x}$. This function with explanation might be found in \cite{Saarinen213}.
\todo{pridat obrazok rautracingu}


%Ray tracing line starts at point $x_{s}$ and ends in point $y_{i}$. We define our line in slope-intercept form with parameter $t \in \R $ and $l_{o}$ is a point on the line:
%\begin{equation}
%x(t) = \frac{y_{i} - x_{s}}{ \lVert y_{i} - x_{s} \lVert } \:t + l_{o}
%\end{equation}
%Given a normal distribution $N(\mu_{i}, P_{i})$ from the cell hit by ray-tracing, the likelihood along the line is defined as function:

\newpage   
\section{Scan registration}
\label{Scan_reg}
Scan registration is one of the key concepts in full SLAM solution. Algorithm can use scan matching between two scans to determine transformation. It tells how far robot moved between scans. Two scans might not offer enough information for successful registration. Imagine a robot which is standing in the corner of a room with sensor facing the wall. Scan from this robot has only information from very limited field of view and this might lead to matching errors. Therefore, it is usually necessary to combine individual scans to operate with more data.  

One of the algorithms which uses this process is called incremental scan-matching. It takes arriving scan and tries to match it against the map built from previous measurements. By doing so it can very well be used instead of robots odometry in \gls{SLAM}'s graph creation. Algorithms which are possible to work in incremental scan-matching are mentioned in sections \ref{subsec:P2D_NDT} and \ref{subsec:D2D_NDT}. Other often used approach is the \gls{ICP}, which is described in \ref{subsec:ICP}. All these algorithms use optimization methods, e.g. Newton's method. Good initial guess is needed in order to guarantee converge to the right solution. 

Another example of usage scan registration in \gls{SLAM} is for testing generated loop closures. Loop closures are created by \gls{SLAM}'s front-end as mentioned in section \ref{Pose_graph_creation}. Measurements from nodes which play role in loop closure are scan matched. By doing this we are trying to proof if two nodes are really overlapping.  The Biggest problem with this registration is that we have no valid prior information about positions of these nodes. These two scans might be perfectly aligned or they can be from completely different parts of the world. Registration needs to robustly estimate the transformation. In case of misleading closure algorithm should reject it. One scan-matcher capable of robust transformation calculation is mentioned in \ref{subsec:Corr}.  


Even robust scan-matchers can fail to correctly identify loop closures.  These registration mismatches can be divided into two categories.

The First category is local ambiguity. Good example of it is when robot moves in long corridor as seen in figure \ref{Pic_coridor} on the left. Environment does not have many distinctive features and algorithm selected one of three possible correct options.

The second category is global ambiguity. This ambiguity usually happens when algorithm do not have enough information about whole environment. Limited size of scans and environment with similar indistinguishable elements can resolve in wrong association. One example can be seen in figure \ref{Pic_coridor} on the right.   
     
 \begin{figure}
 \label{Pic_coridor}

 \end{figure}

  \todo{picture of corridor}
  \newpage
\subsection{NDT registration}
\label{subsec:P2D_NDT}
NDT registration process was first time explained by \cite{Biber03}. They have explained how to make 2D registration between older scan (target scan) and newer scan (source scan). Target scan was converted to NDT grid by technique mentioned in section \ref{subsec:NDT_grid}. Result of registration should be transformation defined in 2D:
\begin{equation}
T: 
\begin{pmatrix}
x' \\ y'
\end{pmatrix}
=
\begin{pmatrix}
\cos \theta  & -\sin \theta\\
\sin \theta & \cos \theta
\end{pmatrix}
\begin{pmatrix}
x \\ y
\end{pmatrix}
+
\begin{pmatrix}
t_{x} \\ t_{y} 
\end{pmatrix}
\end{equation}
where $ (t_{x},t_{y})^{T}$ represents translation and $\theta$ represents rotation. Transformation is used for transforming source scan. At the beginning of program parameters of transformation are initialized either by zero or from initial guess. For each point of transformed scan cost function is computed This function is defined as:
\begin{equation}
score(\textbf{p}) = \sum_{i}^{} \exp(-\frac{1}{2} ((T(x_{i},\textbf{p})- \mu_{i})^{T} P^{-1}_{i}   (T(x_{i},\textbf{p})- \mu_{i}) ) 
\end{equation}
where $\textbf{p} = (t_{x},t_{y}, \theta)$ are parameters of transformation, $N(\mu_{i},P_{i})$ are parameters of normal distribution where point $x$ is transformed by transformation $T$.Goal of the NDT scan-matching is to find parameters $\textbf{p}$ which maximize this function. This maximization problem is changed to minimization problem by searching for minimal value of -score. Newton's algorithm finds minimizing parameters in $p$ by iteratively solving equation 
\begin{equation}
 H \varDelta p = -g
\end{equation}  

Representation of hessian, gradient and all derivations might be found in work of \cite{magnusson09}. Magnusson also introduced new scaling parameters into score function in order to reject possible outliers. \gls{PDF} inside of cells of target \gls{NDT} grid may not be always from normal distribution. In practice any representation which approximates structure of the element is valid. Outliers are points far from the mean of distribution and cause unbounded growth of \gls{PDF}.

At the beginning, algorithm created discrete \gls{NDT} grid out of target scan. This introduces discretization problems. These problems are cause by points generating \gls{PDF} which are larger than their cells. In the original work of \cite{Biber03} this was solved by creating 4 target grids where each grid is translated by half of the cell size in single direction. This process made this algorithm inefficient. Introduction to multi-layer \gls{NDT} grid structure, presented by \cite{ulas20113d}, solved this problem. Multi-layer approach consists of several grids with different resolution. Grids are ordered from coarser grid to finer grid. Algorithm starts with coarse grid and estimates parameters of transformation. Calculated transformation is used as initial guess at lower level. This principle practically eliminated need for four overlapping grids. It also offered better convergence time and increase to robustness. Algorithm is able to converge when matched scans are farther away. Good configuration is 4 layers with cell sizes 2, 1, 0.5 and 0.25 meters. 

Another improvement to algorithm is usage of concept of linked cells. In practical registration very often part of the source scan lie far from any target cells. This causes only small portion of points contribute to score function. It can cause algorithm failure or just increase time of convergence. Linked cells prevent this by providing cells in target scan, which are close to the point from source scan. Implementation of this technique is possible with use of kD-tree with means of all cells as input points. Every point or source scan finds k-nearest cells and execute score calculation on them.

\begin{algorithm}
\label{alg:p2d_ndt}
    \caption{\gls{NDT} algortihms with muilti layer and linked cell enhancements}
\begin{algorithmic}[1]
 \Require source scan, target scan,  parameters $(x,y,\theta)$ of initial transformation, cell resolution for each layer
 \Function{NDTRegistration}{$scan_{s}$, $scan_{t}$, $p_{init}$,$resolutions$}
	 \State $\textbf{p} \gets p_{init}$
	 \ForAll{$res$ from $resolutions$}
		 \State $ndt_{t} \gets$createNDTGrid($res$,$scan_{t}$)
		 \Comment described in section \ref{subsec:NDT_grid}
		 \State transform each point $x_{i} \in scan_{trans}$ with $T(x_{i},\textbf{p})$
		 \State $\textbf{p} \gets$ computeSingleGrid($scan_{trans}$, $ndt_{t}$, $\textbf{p}$)
	 \EndFor
	 \State \Return calculated parameters $\textbf{p}$ of transformation
 \EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\label{alg:p2d_ndt_single}
\caption{Computing transformation on with single target \gls{NDT} grid and source point cloud}
\begin{algorithmic}[1]
 \Require source scan, target NDT grid,  parameters $(x,y,\theta)$ of initial transformation
 \Function{computeSingleGrid}{$scan_{s}$, $ndt_{t}$,$p_{init}$}
	 \While {not converged}
		 \State $\textbf{p} \gets p_{init}$
		 \State $(score, g,H) \gets (0,0,0)$
		 \ForAll {points $x_{i} \in scan_{trans}$}
			 \State $\bar{x_{i}} \gets T(x_{i},\textbf{p})$
			 \State $cells \gets $find k-closest cells to $\bar{x_{i}} $
			 \ForAll{cells $c_{i} \in cells$}
			 \State \{based on \cite{magnusson09}\}
				 \State $(score,g,H) \gets (score,g,H)  + $ calcNewtonParameters($c_{i}$,$\bar{x_{i}}$)
			 \EndFor 
		 \EndFor
		 \State solve $H\varDelta p = - g$
		 \State $\textbf{p} \gets \textbf{p} + \varDelta p$
	 \EndWhile
	 \State \Return $\textbf{p}$
 \EndFunction
\end{algorithmic}
\end{algorithm}
         
 
\newpage
\subsection{D2D-NDT registration}
\label{subsec:D2D_NDT}
\gls{D2D}-\gls{NDT} is variant of \gls{NDT} registration algorithm proposed by \cite{Stoyanov01102012}. It is extension of original algorithm presented in section \ref{subsec:P2D_NDT}. Instead of using only one grid for target scan. This aproach uses two grids. One for source scan and second for target grid. Algorithm than minimize the sum of $L_{2}$ distances between pairs of \gls{PDF}'s from both grids. Formally, transformation between two sets of cells $X$ and $Y$ is defined as:
\begin{equation}
f(\textbf{p}) = \sum_{i = 1, j =1}^{n_{X}, n_{Y}} -d_{1}\exp\left( - \frac{d_{2}}{2}  \mu_{ij}^{T}(R^{T}P_{i}R + P_{j})^{-1} \mu_{ij} \right)
\end{equation}
\begin{equation}
\label{eq:D2D_2}
\mu_{ij} = R\mu_{i} + t - \mu_{j}
\end{equation}
where $\textbf{p} = (t_{x},t_{y},\theta)$; $X(\mu_{i},P_{i})$ and $Y(\mu_{j}, P_{j})$ are \gls{PDF}'s of individual cells in pair; a pair $(R,t)$ represents rotation matrix from parameter $\theta$ and translation vector $t=(t_{x},t_{y})$. Regulation parameters d1 and d2 are set to values $d_{1} = 1$ and $d_{2}=0.05$. Equation \ref{eq:D2D_2} represents difference in means where mean $u_{i}$ is transformed to new position.

Optimization of this function is done in similar way to \ref{subsec:P2D_NDT}  by utilizing Newtons method and solving $H\varDelta \textbf{p} = -g$. Derivations for calculation of hessian and gradient are presented in work of \cite{Stoyanov01102012}.

This algorithm is also possible to improve by iterating over multiple layers with different resolutions similar to \gls{NDT} registration in previous section.

In comparison, with \gls{NDT} registration this algorithm needs only \gls{NDT} grids for registration. Point cloud can be thrown away after successful creation of grid. This allow saving memory and efficiently represent maps in \gls{SLAM}. In addition, \gls{D2D} is almost ten times faster than standard \gls{NDT} registration on same dataset. This was proven in comparative study from \cite{NDTcomparative}. The main cause of this speed up is smaller number of calls for score calculation. In \gls{P2D}-\gls{NDT} mentioned in last section we need to calculate score for each point in source point cloud. In case of \gls{D2D} we just calculate score function for each cell of source grid. This is done by generating only pairs between cell from source grid and closest cell from target cell. Closest cell can be easily found by using kD-tree with values of target grid's means.
  
\subsection {ICP}
\label{subsec:ICP}
The iterative closest point (\gls{ICP}) algorithm was first introduced by \cite{chen92ICP} and it is still very popular method for registering point clouds. To briefly summaries algorithm: ICP iteratively refines position of two point clouds by optimizing the sum of square distances between corresponding pair of points from two clouds. This approach is usually called point-to-point registration. Class of algorithms based on \gls{ICP} has developed many modifications. Surrvey of base type of ICP algorithms and their comparison on well designed datasets is in work of \cite{pomerleau2013comparing}. 


\newpage
\subsection{Correlative scan registration}
\label{subsec:Corr}
Correlative scan registration is algorithm presented by \cite{olson2009real}. This method was developed to robustly solve registration problem. It does not require any initial guess. Therefore, it is possible to use it for loop closure registration.

 The algorithm requires two point clouds. Target point cloud is used for generation of fast look up table filled with bit values. It is created by separating points from target cloud into individual cells. Every cell which has some points in it is marked as occupied. After this step we have a table with value 1 in cells with some points and value 0 in cells without points. In next step we add sensor noise to the table. As a function of our noise we use radially symmetric kernel. 
 \begin{equation}
 K_{i,j} = \exp\left(\frac{-1}{2}\left(\frac{\sqrt{(i r)^{2} + (j r)^{2}}}{\sigma}\right)^{2}\right) \eta
 \end{equation}
 \begin{equation}
 K =
 \begin{pmatrix}
  2 & 14 & 2\\
  14 & 100 & 14 \\
  2 & 14 & 2 
 \end{pmatrix}
 \end{equation}
 where $K_{i,j}$ is one element of kernel; $\sqrt{(i r)^{2} + (j r)^{2}}$ is euclidean distance from center of the kernel to the element $i,j$ with cell size parameter $r$. Standard deviation of sensor nose is abbreviated by $\sigma$ and $\eta$ is kernels max value.

 The Kernel overlaps over every occupied cell in the table. If value of kernel is higher than value in table. Table is updated with the kernels value. Generated smoothing can be seen in figure \ref{fig:Corr_smooth}. \todo{figure smoothing or remove}
 
 This algorithm is avoiding initial guess by trying all possible rotations and translations of source cloud. Every point of transformed source cloud is mapped into certain cell of look up table. The total score of transformed cloud is sum of all mapped cells scores. Algorithm usually tries rotations and translations from selected range. Transformation with the best score is the most probable transformation.
 
 This brute force process might take long time if we select small cell size to achieve good registration. To speed up this process we first need to avoid computationally expensive calculation of goniometric functions in transformation. This can be achieved by first generating all possible rotations of point cloud. For each rotation we try all translations from selected range with step size selected based on cell size of look up table.
 
 Real speed improvements offers usage of two layer architecture of look up tables. The first table has coarse resolution. This table is used for initial estimation on the whole range of selected rotations and translations. The transformations with best score are used in the second round. From every good transformation is generated search space voxel. Origin of voxel is taken from transformation. Size of voxel is cell size from coarse table. Search voxels are evaluated on look up table with fine resolution. Search space is this time limited to search voxel and initial transformation is taken from origin of voxel. The best result is our solution. By this process computation time drops rapidly as show in work of \cite{olson2009real}.     
    
\newpage
\section{NDT graph based SLAM motivation}
\label{subsec:analyses}
In previous sections we have presented several algorithms which are currently considered state-of-the-art. After this research we have noticed that there are no implementation of \gls{NDT} based graph \gls{SLAM} in \gls{ROS}. In our advantage, some algorithms needed for this development are already well documented and tested in reaserch works of other authors. This works should build on this foundation. 

In the conclusion of work by \cite{NDTOMFusion}, author described that scanmatching based on \gls{NDT} registration can provide very good result in mapping process with use of \gls{NDT-OM}. They have proven this by mapping large area with only use of global map solely created by modified method of  incremental scanmatching. They have noted that even though results are very accurate, there is need for solution with loop closure mechanism to improve results. In this work, they have mapped real life factory datasets with dynamic entities. \gls{NDT-OM} proven to be reliable way of removing dynamic objects and updating map. \gls{NDT} maps have in general advantage of reliable registration performance on coarser grid. Which is valuable trait for reduction of memory consumption. At the same type it holds occupancy information which can be used by planning and exploring algorithms.

It is necessary to find solution to the way how to represent the map. In previous works there was always one global map. Iterative scanmatching than used this map for more precise alignment. This needs to be reanalyzed, because with use of pose graph we need a way how to reorganize map based on optimization changes in \gls{SLAM}'s back-end.  

These changes to pose graph happen after valid loop closure detection. To perform alignment we need algorithm which can align two grids without information about initial guess. In addition, this algorithm should offer a way how to grade registration based on some quality metric. This should reject edges which cover completely different location.

Lastly, it needs to be implemented in a way that on-line processing of real datasets is possible. It needs to have standard \gls{ROS} interface commonly found in other \gls{SLAM}. It should use standard libraries available in \gls{ROS}.

Final result of this works should be working implementation of 2D graph based \gls{SLAM} on NDT maps with easy use inside of \gls{ROS} ecosystem. 

     

